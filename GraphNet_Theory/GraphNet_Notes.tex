\section{Graph Networks}
\subsection{Input}
\begin{itemize}
	\item Feature matrix $N x F^{0}, X$
	\item $N$ is the number of nodes
	\item $F^{0}$ is the number of input features for each node
	\item $N x N$ matrix representation of the graph structure (adjacency matrix)
\end{itemize}
\subsection{Simple Propagation Rule}
\begin{itemize}
	\item $f(H^{I}, A) = \sigma(AH^{I}W^{I})$
	\item $W^{I}$ is the weight matrix for layer $i$ (dimensions $F^{I} x F^{i+1}$)
	\item $\sigma$ is a non-linear activation function (ReLU function)
	\item Size of second dimension of weight matrix determines number of features at next layer
\end{itemize}
\subsubsection{Issues with Propagation Rule}
\begin{itemize}
	\item Does not inlcude its own features
	\item Representation only includes neighbor nodes unless has a self-loop
	\item Nodes with large degrees will have large values and small degrees will have smaller values in representation
	\item Causes vanishing or exploding gradients (problematic stochastic gradient descent)
\end{itemize}
\subsubsection{Solving Propagation Rule Issues}
\begin{itemize}
	\item Add identity matrix to adjacency matrix before applying propagation rule to include its own features
	\item Normalize feature representations by multiplying adjacency matrix by its inverse degree matrix
\end{itemize}
\subsection{Graph Neural Network Transition Functions}
\subsubsection{Branched Fixed Point Theorem and Jacobi Method}
\begin{itemize}
	\item There exists a unique solution to a system of equations and provides a method to compute those fixed points
	\item If assuming there is a metric space $X$ and a mapping $T: X \rightarrow X$
	\item Mapping is a contraction mapping on $X$ in which $T$ admits a unique fixed point $x^{*}$ in $X$ (e.g. $T(x^{*}) = x^{*}$)
	\item GNN transition function is assumed to be a contractive mapping with respect to the nodes' state
	\item Since BFP guarantees a unique solution, Jacobi iterative method is used to solve the fixed point solution, the nodes' state
	\item Solved by approximating values to plug in and then iterating until convergence
\end{itemize}
\subsubsection{Message Passing Neural Network}
\begin{itemize}
	\item Does not assume edge features are discrete
	\item Message Phase
\begin{itemize}
	\item Transition function and output function combined where $M$ is the transition function and $U$ is the output function
	\item $m_{n}^{t+1} = \sum_{u \in ne[n]}M_{t}(h_{n}^{t}, h_{u}^{t}, e_{(n,u)})$
	\item $h_{n}^{t+1}=U_{t}(h_{n}^{t}, m_{n}^{t+1})$
\end{itemize}
	\item Readout Phase
\begin{itemize}
	\item Function of all the nodes' states and outputs a label for the entire graph
	\item $\hat{y} = R(\{h_{n}^{T} | n \in G\})$
\end{itemize}
\end{itemize}
\subsection{Spectral Graph Convolutions}
\begin{itemize}
	\item $f(X, A) = \sigma (D^{-0.5} \hat{A}D^{-0.5}XW)$
	\item Normalizes aggregate using the degree matrix $D$ raised to a negative power (asymmetric)
	\item Considers degree of $ith$ notde but also degree of $jth$ node
	\item Weighs neighbor in the weighted sum higher if they have a lower degree and lower if they have a high-degree
\end{itemize}
\subsubsection{Semi-Supervised Classification with GCNs}
\begin{itemize}
	\item Make use of both labeled and unlabled examples
	\item Nodes that share neighbors tend to have similar feature representations
	\item Train GCN on labeled nodes and effectively propogate node label info to unlabled nodes b updating weight matrices that are shared across all nodes:
\begin{itemize}
	\item Perform forward propagation through GCN
	\item Apply sigmoid function row-wise on the last layer in the GCN
	\item Compute cross entropy loss on known node labels
	\item Backpropogate loss and update weight matrices W in each layer
\end{itemize}
\end{itemize}

https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0